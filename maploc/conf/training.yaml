experiment:
  name: ???
  gpus: 4
  seed: 0
  base_batch_size: 4
training:
  lr: 1e-4
  lr_scheduler:
    name: CosineAnnealingLR
    warmup_steps: 1000
    max_epochs: ${..trainer.max_epochs}
    T_max: 5
    eta_min: 1e-6
  finetune_from_checkpoint: null
  trainer:
    val_check_interval: 1000
    log_every_n_steps: 500
    limit_val_batches: 1000
    max_epochs: 5
    devices: ${experiment.gpus}
  checkpointing:
    monitor: "loss/total/val"
    save_top_k: 3
    mode: "min"
  early_stopping:
    monitor: loss/total/val
    patience: 1
    mode: min
    min_delta: 0.001
hydra:
  job:
    name: ${experiment.name}
    chdir: false